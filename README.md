RAG (Retrieval-Augmented Generation) - Sistema de Q&A em PortuguÃªs

Sistema de perguntas e respostas baseado em RAG (Retrieval-Augmented Generation) que utiliza documentos PDF como base de conhecimento para responder perguntas em portuguÃªs sobre Java. 

ğŸ“‹ DescriÃ§Ã£o

Este projeto implementa um pipeline RAG que:
- Carrega documentos PDF
- Divide o conteÃºdo em chunks menores
- Cria embeddings vetoriais dos documentos
- Armazena os embeddings em um banco vetorial FAISS
- Recupera informaÃ§Ãµes relevantes para responder perguntas
- Utiliza um LLM (Llama 3.3) via Groq para gerar respostas contextualizadas

ğŸ› ï¸ Tecnologias Utilizadas

- **LangChain**: Framework para desenvolvimento de aplicaÃ§Ãµes com LLMs
- **Groq**: API para acesso ao modelo Llama 3.3 70B
- **HuggingFace**: Embeddings usando `sentence-transformers/all-MiniLM-L6-v2`
- **FAISS**: Banco de dados vetorial para busca semÃ¢ntica
- **Python-dotenv**: Gerenciamento de variÃ¡veis de ambiente

ğŸ“¦ InstalaÃ§Ã£o

1. Clone o repositÃ³rio: 
```bash
git clone https://github.com/eliswilliam/RAG.git
cd RAG
```

2. Instale as dependÃªncias:
```bash
pip install langchain-groq langchain-huggingface langchain-community
pip install faiss-cpu python-dotenv pypdf
```

3. Configure as variÃ¡veis de ambiente: 
Crie um arquivo `.env` na raiz do projeto com sua chave da API Groq:
```
GROQ_API_KEY=sua_chave_api_aqui
```

## ğŸš€ Uso

1.  Coloque seus arquivos PDF na raiz do projeto (atualmente configurado para `java (1).pdf`)

2. Execute o script: 
```bash
python main_rag.py
```

3. Para fazer suas prÃ³prias perguntas, modifique a Ãºltima linha do cÃ³digo: 
```python
print(responder("Sua pergunta aqui? "))
```

## ğŸ”§ ConfiguraÃ§Ãµes

### Ajustar o Modelo LLM
```python
modelo = ChatGroq(
    model="llama-3.3-70b-versatile",  # Modelo utilizado
    temperature=0.5,                   # Ajusta a criatividade (0-1)
    groq_api_key=groq_api_key
)
```

### Ajustar Chunking
```python
pedacos = RecursiveCharacterTextSplitter(
    chunk_size=1000,    # Tamanho de cada pedaÃ§o
    chunk_overlap=100   # SobreposiÃ§Ã£o entre pedaÃ§os
).split_documents(documentos)
```

### Ajustar RecuperaÃ§Ã£o
```python
dados_recuperados = FAISS.from_documents(
    pedacos, embeddings
).as_retriever(search_kwargs={"k":2})  # NÃºmero de chunks retornados
```

## ğŸ“‚ Estrutura do Projeto

```
RAG/
â”œâ”€â”€ main_rag.py          # Script principal
â”œâ”€â”€ java (1).pdf         # Documento fonte (exemplo)
â”œâ”€â”€ .env                 # VariÃ¡veis de ambiente (nÃ£o versionado)
â””â”€â”€ README.md            # Este arquivo
```

## ğŸ” Como Funciona

1. **Carregamento**:  PDFs sÃ£o carregados usando `PyPDFLoader`
2. **DivisÃ£o**:  Documentos sÃ£o divididos em chunks de 1000 caracteres com overlap de 100
3. **Embedding**: Cada chunk Ã© convertido em vetor usando modelo HuggingFace
4. **Armazenamento**:  Vetores sÃ£o armazenados no FAISS para busca eficiente
5. **RecuperaÃ§Ã£o**: Ao fazer uma pergunta, o sistema busca os 2 chunks mais relevantes
6. **GeraÃ§Ã£o**: O LLM gera uma resposta baseada exclusivamente no contexto recuperado

## ğŸ¯ Exemplo de Uso

```python
from main_rag import responder

# Fazer uma pergunta
resposta = responder("O que Ã© uma classe abstrata em Java?")
print(resposta)
```

## ğŸ“ Notas

- O sistema responde **exclusivamente** com base no conteÃºdo dos documentos fornecidos
- A temperatura estÃ¡ configurada em 0.5 para balancear criatividade e precisÃ£o
- O modelo de embeddings Ã© otimizado para textos em portuguÃªs

## ğŸ”‘ Obtendo a API Key do Groq

1. Acesse [console.groq.com](https://console.groq.com)
2. FaÃ§a login ou crie uma conta
3. Navegue atÃ© a seÃ§Ã£o de API Keys
4. Crie uma nova chave e copie para seu arquivo `.env`

## ğŸ¤ Contribuindo

ContribuiÃ§Ãµes sÃ£o bem-vindas!  Sinta-se Ã  vontade para abrir issues ou pull requests.



Desenvolvido por [@eliswilliam](https://github.com/eliswilliam)
